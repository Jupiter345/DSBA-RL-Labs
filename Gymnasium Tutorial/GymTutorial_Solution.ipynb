{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Creating Gym Environments\n",
        "\n",
        "This notebook will help you an introduction on how to create RL environments using the [Gymnasium](https://gymnasium.farama.org/) library. \n",
        "\n",
        "Please take a look at the documentation of the library before starting."
      ],
      "metadata": {
        "id": "nSGqIb7VumSC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUembeTjpanG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fc2f1c-e5d0-406f-a0fc-6bccda1419ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.27.1-py3-none-any.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (4.5.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (2.2.1)\n",
            "Collecting jax-jumpy>=0.2.0\n",
            "  Downloading jax_jumpy-0.2.0-py3-none-any.whl (11 kB)\n",
            "Collecting gymnasium-notices>=0.0.1\n",
            "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (1.22.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium) (3.15.0)\n",
            "Installing collected packages: gymnasium-notices, jax-jumpy, gymnasium\n",
            "Successfully installed gymnasium-0.27.1 gymnasium-notices-0.0.1 jax-jumpy-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import IPython\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "YGbTkqOQpgoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Definition\n",
        "\n",
        "To illustrate the process of creating a gym environement using the gymnasium library, we will implement a simplistic GridWorld. The environment consists of a 2-dimensional square grid of fixed size (specified via the size parameter during construction). The agent can move vertically or horizontally between grid cells in each timestep. The goal of the agent is to navigate to a target on the grid that has been placed randomly at the beginning of the episode. There is catch however, some cells are DANGEROUS, this means that when a player takes an action within be in one of the dangerous cells there is a $\\alpha$ probability of succeding the desired action and a probability $1-\\alpha$ to take another random action instead (slipped).\n",
        "\n",
        "Observations provide the location of the target the agent and the locations of the dangerous cells.\n",
        "\n",
        "There are 4 actions in our environment, corresponding to the movements “right”, “up”, “left”, and “down”.\n",
        "\n",
        "A done signal is produced as soon as the agent has navigated to the grid cell where the target is located.\n",
        "\n",
        "Rewards are binary and sparse, meaning that the immediate reward is always zero, unless the agent has reached the target, then it is 1.\n",
        "\n",
        "An episode in this environment (with size=5) might look like this when rendered in ansi mode:\n",
        "\n",
        "```\n",
        "+-----+\n",
        "|...A.|\n",
        "|D....|\n",
        "|....D|\n",
        "|..TD.|\n",
        "|.....|\n",
        "+-----+\n",
        "```\n",
        "where the `A` is the agent and the `T` square represents the target and the `D` squares the perils.\n",
        "\n",
        "Let us look at the source code of DangerousWorld piece by piece:"
      ],
      "metadata": {
        "id": "x5RpgMC1saju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DangerousWorld(gym.Env):\n",
        "  # These metadata are required from the gym and are related to rendering\n",
        "  \"\"\"\n",
        "  The DangerousWorld environment consists of a 2-dimensional square grid of \n",
        "  fixed size (specified via the size parameter during construction). The agent \n",
        "  can move vertically or horizontally between grid cells in each timestep. The \n",
        "  goal of the agent is to navigate to a target on the grid that has been placed \n",
        "  randomly at the beginning of the episode. There is catch howver, some cells \n",
        "  are dangerous, this means that when a player takes an action within be in one \n",
        "  of the dangerous cells there is a $alpha$ probability of succeding the desired \n",
        "  action and a probability  $1−alpha$  to take another random action instead.\n",
        "  \"\"\"\n",
        "  metadata = {\"render_modes\": [\"ansi\", \"rgb\"], \"render_fps\": 4}\n",
        "\n",
        "  def __init__(self, size, alpha=0.5, n_dangers=3, render_mode=\"ansi\"):\n",
        "    \"\"\"\n",
        "      Constructor for the DangerousWorld\n",
        "      Parameters:\n",
        "              size (int): size of the gridworld\n",
        "              alpha (float): probability of kept alive on dangerous cells\n",
        "              n_dangers (int): number of dangerous cells\n",
        "    \"\"\"\n",
        "\n",
        "    # Checks if the number of dangerous cells are fitting in the grid\n",
        "    assert n_dangers < size**2+1, f\"n_dangers should be at most {size**2}\"\n",
        "\n",
        "    self.render_mode = render_mode\n",
        "    self.size = size\n",
        "    self.alpha = alpha\n",
        "    self.n_dangers = n_dangers\n",
        "\n",
        "    # Create the self.action_space and an self.observation_space for the grid \n",
        "    # world using the spaces functionality of gymnasium (https://gymnasium.farama.org/api/spaces/)\n",
        "    # for the observation space you can use a dictionary structure that holds \n",
        "    # the locations of the agent, target and dangerous locations. You can use the \n",
        "    # spaces.Box, spaces.Discrete and spaces.Dict.\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    self.observation_space = gym.spaces.Dict(\n",
        "        {\n",
        "            \"agent\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
        "            \"target\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
        "            \"is_dangerous\": gym.spaces.Tuple([gym.spaces.Box(0, size - 1, shape=(2,), dtype=int) for _ in range(n_dangers)])\n",
        "        }\n",
        "    )\n",
        "    # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
        "    self.action_space = gym.spaces.Discrete(4)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    \"\"\"\n",
        "    The following dictionary maps abstract actions from `self.action_space` to\n",
        "    the direction we will walk in if that action is taken.\n",
        "    I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
        "    \"\"\"\n",
        "    self._action_to_direction = {\n",
        "        0: np.array([1, 0]),\n",
        "        1: np.array([0, 1]),\n",
        "        2: np.array([-1, 0]),\n",
        "        3: np.array([0, -1]),\n",
        "    }\n",
        "\n",
        "  def reset(self, seed=None, options=None):\n",
        "    # We need the following line to seed self.np_random\n",
        "    super().reset(seed=seed)\n",
        "\n",
        "    # Sample unique n_dangerous (+2 for agent and target) locations using the \n",
        "    # function self.np_random.integers() function in order to populate the \n",
        "    # locations for all the elements of the gridworld and store them at\n",
        "    # self._agent_location, self._target_location and self._is_dangerous \n",
        "    # variables.\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    locations = []\n",
        "    while len(locations) < 2 + self.n_dangers:\n",
        "      loc = list(self.np_random.integers(0, self.size, size=2, dtype=int))\n",
        "      if not loc in locations:\n",
        "        locations.append(loc)\n",
        "\n",
        "    self._agent_location = np.array(locations[0])\n",
        "    self._target_location = np.array(locations[1])\n",
        "    self._is_dangerous = tuple([np.array(l) for l in locations[2:]])\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    self.last_action = None\n",
        "    self.dead = False\n",
        "\n",
        "    # initialize level paramters related to the rgb render\n",
        "    if self.render_mode=='rgb':\n",
        "      self._init_rgb_render()\n",
        "\n",
        "    # returns observations and info.\n",
        "    observation = self._get_obs()\n",
        "    info = self._get_info()\n",
        "\n",
        "    return observation, info\n",
        "\n",
        "  # Implement the two private functions [_get_obs() and _get_info()] which \n",
        "  # return the current observation and a dictionary with info (e.g. the \n",
        "  # distance between) the target and location. Be careful, the observation\n",
        "  # should follow the structure of the observation space.\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  def _get_obs(self):\n",
        "    return {\n",
        "        \"agent\": self._agent_location, \n",
        "        \"target\": self._target_location,\n",
        "        \"is_dangerous\": self._is_dangerous\n",
        "    }\n",
        "\n",
        "  def _get_info(self):\n",
        "    return {\n",
        "        \"distance\": np.linalg.norm(\n",
        "            self._agent_location - self._target_location, ord=1\n",
        "        )\n",
        "    }\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  def step(self, action):\n",
        "    self.dead = False\n",
        "    self.last_action = action\n",
        "    # Check if standing on dangerous cell. If so, perform the selected action\n",
        "    # with probability self.alpha otherwise terminate and set the reward \n",
        "    # accordingly.\n",
        "    ### START CODE HERE ###\n",
        "    if list(self._agent_location) in [list(i) for i in self._is_dangerous]:\n",
        "      if self.np_random.random() < self.alpha:\n",
        "        a = self.action_space.sample()\n",
        "      else:\n",
        "        self.dead = True\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Agent falls dead and therefore cannot move\n",
        "    if not self.dead:\n",
        "      # Map the action (element of {0,1,2,3}) to the direction we walk in using\n",
        "      # the self._action_to_direction\n",
        "      direction = self._action_to_direction[action]\n",
        "      # We use `np.clip` to make sure we don't leave the grid\n",
        "      self._agent_location = np.clip(\n",
        "          self._agent_location + direction, 0, self.size - 1\n",
        "      )\n",
        "    \n",
        "    # Populate all the variables that are returned from the function\n",
        "    ### START CODE HERE ###\n",
        "    observation = self._get_obs()\n",
        "    terminated = np.array_equal(self._agent_location, self._target_location) or self.dead\n",
        "    reward = -100 if self.dead else -1  # Binary sparse rewards\n",
        "    info = self._get_info()\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return observation, reward, terminated, False, info\n",
        "\n",
        "  def render(self):\n",
        "    if self.render_mode=='ansi':\n",
        "      return self._render_text()\n",
        "    if self.render_mode=='rgb':\n",
        "      return self._render_rgb()\n",
        "\n",
        "\n",
        "  def _render_text(self):\n",
        "    r = np.empty((self.size,self.size), dtype=str)\n",
        "    r[:] = '.'\n",
        "    for i in range(len(self._is_dangerous)):\n",
        "      r[self._is_dangerous[i][0],self._is_dangerous[i][1]] = 'D'\n",
        "    r[self._target_location[0],self._target_location[1]] = 'T'\n",
        "    # if agent over dangerous tile change character\n",
        "    if list(self._agent_location) in [list(i) for i in self._is_dangerous]:\n",
        "      r[self._agent_location[0],self._agent_location[1]] = 'Ã'\n",
        "    else:\n",
        "      r[self._agent_location[0],self._agent_location[1]] = 'A'\n",
        "    self._render = r\n",
        "\n",
        "    # Render borders for aesthetic reasons\n",
        "    rr = np.pad(r, ((1,1),(0,0)), 'constant', constant_values=('-')) \n",
        "    rrr = np.pad(rr, ((0,0),(1,1)), 'constant', constant_values=('|'))\n",
        "    rrr[0,0] = \"+\"\n",
        "    rrr[0,-1] = \"+\"\n",
        "    rrr[-1,0] = \"+\"\n",
        "    rrr[-1,-1] = \"+\"\n",
        "    r_str = \"\\n\".join([\"\".join(i) for i in rrr])\n",
        "    if self.last_action is not None:\n",
        "      r_str += f\"\\n({['Down', 'Right', 'Up', 'Left'][self.last_action]})\\n\"\n",
        "\n",
        "    if self.dead:\n",
        "      r_str += \"Dead!\"\n",
        "    return r_str\n",
        "\n",
        "  def _render_rgb(self):\n",
        "    r_rgb = Image.fromarray(self.lvl_img).convert(\"RGBA\")\n",
        "    \n",
        "    tloc = self._target_location+np.array([2,1])\n",
        "    target_rgb_np = np.zeros((r_rgb.size[0],r_rgb.size[1],4), dtype='uint8')\n",
        "    target_rgb_np[\n",
        "        tloc[0]*16:tloc[0]*16+16,\n",
        "        tloc[1]*16:tloc[1]*16+16,\n",
        "        :\n",
        "    ] = self.lvl_target\n",
        "    target_rgb = Image.fromarray(target_rgb_np).convert(\"RGBA\")\n",
        "    r_rgb.alpha_composite(target_rgb)\n",
        "\n",
        "\n",
        "    for i in range(self.n_dangers):\n",
        "      # adding [2,1] to account for borders\n",
        "      sloc = self._is_dangerous[i]+np.array([2,1])\n",
        "      danger_rgb_np = np.zeros((r_rgb.size[0],r_rgb.size[1],4), dtype='uint8')\n",
        "      danger_rgb_np[\n",
        "        sloc[0]*16:sloc[0]*16+16,\n",
        "        sloc[1]*16:sloc[1]*16+16,\n",
        "        :\n",
        "      ] = self.lvl_obstacles[i]\n",
        "      danger_rgb = Image.fromarray(danger_rgb_np).convert(\"RGBA\")\n",
        "      r_rgb.alpha_composite(danger_rgb)\n",
        "\n",
        "    aloc = self._agent_location+np.array([2,1]) \n",
        "    agent_rgb_np = np.zeros((r_rgb.size[0],r_rgb.size[1],4), dtype='uint8')\n",
        "    agent_rgb_np[\n",
        "        aloc[0]*16:aloc[0]*16+16,\n",
        "        aloc[1]*16:aloc[1]*16+16,\n",
        "        :\n",
        "    ] = self.lvl_agent\n",
        "    agent_rgb = Image.fromarray(agent_rgb_np).convert(\"RGBA\")\n",
        "    r_rgb.alpha_composite(agent_rgb)\n",
        "    return r_rgb.resize((300,300),Image.NEAREST)\n",
        "\n",
        "  def _init_rgb_render(self):\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    response = requests.get(\"https://gist.githubusercontent.com/stergioc/28daf8770b4f1bf1622bd1355c8f88d0/raw/cd6903dbfc3b65829bc7eca056b1ed9e4eedf7be/GridWorld_tileset.png\")\n",
        "    tileset = Image.open(BytesIO(response.content)).convert('RGBA')\n",
        "    tileset_np = np.array(tileset)\n",
        "    tiles_16 = []\n",
        "    for i in np.arange(0,tileset_np.shape[0], 16):\n",
        "      for j in np.arange(0,tileset_np.shape[1], 16):\n",
        "        tiles_16.append(tileset_np[i:i+16,j:j+16,:])\n",
        "    self.tiles = {\n",
        "        0 : tiles_16[22:27],  # floor\n",
        "        1 : tiles_16[1:5],    # top\n",
        "        2 : tiles_16[29:34],  # bottom\n",
        "        3 : [tiles_16[7]]+[tiles_16[14]]+[tiles_16[21]], # left\n",
        "        4 : [tiles_16[13]]+[tiles_16[20]]+[tiles_16[27]],# right\n",
        "        5 : [tiles_16[34]],   # bottom right\n",
        "        6 : [tiles_16[6]],    # top right\n",
        "        7 : [tiles_16[28]],   # bottom left\n",
        "        8 : [tiles_16[0]],    # top left\n",
        "        9 : tiles_16[8:11],   # danger\n",
        "        10: tiles_16[11:13] + tiles_16[15:20],  # players\n",
        "        11: tiles_16[35:42],  # walls\n",
        "        12: tiles_16[42:49],  # targets\n",
        "    }\n",
        "\n",
        "    self.lvl_lut = np.zeros((self.size,self.size))\n",
        "    self.lvl_lut = np.pad(self.lvl_lut, ((1,0),(0,0)), 'constant', constant_values=(11)) \n",
        "    self.lvl_lut = np.pad(self.lvl_lut, ((1,0),(0,0)), 'constant', constant_values=(1)) \n",
        "    self.lvl_lut = np.pad(self.lvl_lut, ((0,1),(0,0)), 'constant', constant_values=(2))\n",
        "    self.lvl_lut = np.pad(self.lvl_lut, ((0,0),(1,0)), 'constant', constant_values=(3))\n",
        "    self.lvl_lut = np.pad(self.lvl_lut, ((0,0),(0,1)), 'constant', constant_values=(4))\n",
        "    self.lvl_lut[0,0] = 8\n",
        "    self.lvl_lut[0,-1] = 6\n",
        "    self.lvl_lut[-1,0] = 7\n",
        "    self.lvl_lut[-1,-1] = 5\n",
        "    self.lvl_img = np.zeros(\n",
        "        (self.lvl_lut.shape[0]*16,self.lvl_lut.shape[1]*16,4), \n",
        "        dtype='uint8'\n",
        "    )\n",
        "    for i in range(self.lvl_lut.shape[0]):\n",
        "      for j in range(self.lvl_lut.shape[1]):\n",
        "        idx = self.lvl_lut[i,j]\n",
        "        tt = self.tiles[idx]\n",
        "        self.lvl_img[i*16:i*16+16, j*16:j*16+16, :] = self.np_random.choice(tt)\n",
        "    self.lvl_agent = self.np_random.choice(self.tiles[10])\n",
        "    self.lvl_target = self.np_random.choice(self.tiles[12])\n",
        "    self.lvl_obstacles = self.np_random.choice(\n",
        "        self.tiles[9], \n",
        "        size=self.n_dangers\n",
        "    )"
      ],
      "metadata": {
        "id": "rrRV24N_pxxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing and Using the Environment"
      ],
      "metadata": {
        "id": "nKTRvXo-sFky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializes the environment and resets it.\n",
        "env = DangerousWorld(size=5, alpha=0, n_dangers=3, render_mode='rgb')\n",
        "env.reset(seed=42)\n",
        "env.render()"
      ],
      "metadata": {
        "id": "P2O50texr8fI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "b6839762-864f-4ced-c562-3fe879588ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGBA size=300x300 at 0x7F6987755EB0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = DangerousWorld(size=5, alpha=0.5, n_dangers=3, render_mode='ansi')\n",
        "env.reset(seed=42)\n",
        "\n",
        "out = display(IPython.display.Pretty(''), display_id=True)\n",
        "# iterate\n",
        "while True:\n",
        "\n",
        "    # Select next action\n",
        "    action = env.action_space.sample()  # for an agent, action = agent.policy(observation)\n",
        "\n",
        "    # Appy action and return new observation of the environment\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "    # Render the game\n",
        "    out.update(IPython.display.Pretty(env.render()))\n",
        "    time.sleep(0.5) # FPS\n",
        "\n",
        "    # If player is dead break\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "LaazGRkzlJjG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}